{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# load data and split into train and test sets\n",
    "from load_data import sim_arr\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# reshape from batch, height, width, channel, to batch, channel, height, width\n",
    "sim_arr_transformed = sim_arr.reshape(\n",
    "    sim_arr.shape[0], sim_arr.shape[3], sim_arr.shape[1], sim_arr.shape[2])\n",
    "train_set, test_set = train_test_split(\n",
    "    sim_arr_transformed, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert to tensor\n",
    "train_set = torch.tensor(train_set, dtype=torch.float32)\n",
    "test_set = torch.tensor(test_set, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder class that inherits from PyTorch's nn.Module class.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 poolsize,\n",
    "                 channels,\n",
    "                 kernel_sizes,\n",
    "                 dilations,\n",
    "                 paddings,\n",
    "                 activations):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=channels[i],\n",
    "                out_channels=channels[i + 1],\n",
    "                kernel_size=kernel_sizes[i],\n",
    "                padding=paddings[i],\n",
    "                dilation=dilations[i]))\n",
    "            self.layers.append(activations[i]())\n",
    "            self.layers.append(nn.MaxPool2d(poolsize[i], ceil_mode=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the encoder.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            print(x.shape)\n",
    "            print(layer)\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder class that mirrors the structure of the Encoder using convolution transpose.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 poolsizes,\n",
    "                 channels,\n",
    "                 kernel_sizes,\n",
    "                 dilations,\n",
    "                 paddings,\n",
    "                 activations):\n",
    "        super().__init__()\n",
    "\n",
    "        poolsizes.reverse()\n",
    "        channels.reverse()\n",
    "        kernel_sizes.reverse()\n",
    "        dilations.reverse()\n",
    "        paddings.reverse()\n",
    "        activations.reverse()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            self.layers.append(nn.ConvTranspose2d(\n",
    "                in_channels=channels[i],\n",
    "                out_channels=channels[i + 1],\n",
    "                kernel_size=kernel_sizes[i],\n",
    "                padding=paddings[i],\n",
    "                dilation=dilations[i],\n",
    "                stride=poolsizes[i]))\n",
    "            self.layers.append(activations[i]())\n",
    "        self.layers.append(nn.ConvTranspose2d(in_channels=channels[-2],\n",
    "                                              out_channels=channels[-1],\n",
    "                                              kernel_size=kernel_sizes[-1],\n",
    "                                              padding=paddings[-1],\n",
    "                                              dilation=dilations[-1],\n",
    "                                              stride=poolsizes[-1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the decoder.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder class that inherits from PyTorch's nn.Module class.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 poolsize,\n",
    "                 channels,\n",
    "                 kernel_sizes,\n",
    "                 dilations,\n",
    "                 paddings,\n",
    "                 activations,\n",
    "                 epochs,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 data):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(num_layers,\n",
    "                               poolsize,\n",
    "                               channels,\n",
    "                               kernel_sizes,\n",
    "                               dilations,\n",
    "                               paddings,\n",
    "                               activations)\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(num_layers,\n",
    "                               poolsize,\n",
    "                               channels,\n",
    "                               kernel_sizes,\n",
    "                               dilations,\n",
    "                               paddings,\n",
    "                               activations)\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # data and dataloader\n",
    "        self.data = data\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the autoencoder.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Compute the reconstruction loss.\"\"\"\n",
    "        x = batch\n",
    "        x_hat = self.forward(x)\n",
    "        loss = self.criterion(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def train_model(self, dataloader, device=torch.device('cuda:0')):\n",
    "        \"\"\"Train the autoencoder.\"\"\"\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self._get_reconstruction_loss(batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item() * batch.size(0)\n",
    "            #epoch_loss = running_loss / len(dataloader)\n",
    "            #print(f\"Epoch {epoch+1}/{self.epochs} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    def evaluate_model(self, dataloader, device=torch.device('cuda:0')):\n",
    "        \"\"\"Evaluate the autoencoder.\"\"\"\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                loss = self._get_reconstruction_loss(batch)\n",
    "                total_loss += loss.item() * batch.size(0)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        #print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    def cross_val(self, n_splits=5):\n",
    "        \"\"\"Perform cross-validation on the autoencoder.\"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "        val_losses = []\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(self.data)):\n",
    "            print(f\"Fold {fold+1}/{n_splits}\")\n",
    "            train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
    "            val_sampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                self.data, sampler=train_sampler)\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                self.data, sampler=val_sampler)\n",
    "\n",
    "            self.train_model(train_loader)\n",
    "            val_loss = self.evaluate_model(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "        return val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_space(trial, input_dim, output_dim):\n",
    "    \"\"\" define the hyperparameter search space.\"\"\"\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    if num_layers == 2:\n",
    "        poolsize = trial.suggest_categorical('poolsize_2', [[5, 16], [16, 5],\n",
    "                                                            [8, 10], [10, 8],\n",
    "                                                            [4, 20], [20, 4],\n",
    "                                                            [2, 40], [40, 2]])\n",
    "    elif num_layers == 3:\n",
    "        poolsize = trial.suggest_categorical(\n",
    "            'poolsize_3', [[2, 2, 20], [2, 20, 2], [20, 2, 2],\n",
    "                           [4, 4, 5], [4, 5, 4], [5, 4, 4],\n",
    "                           [2, 5, 8], [2, 8, 5], [5, 2, 8],\n",
    "                           [5, 8, 2], [8, 2, 5], [8, 5, 2],\n",
    "                           [2, 4, 10], [2, 10, 4], [4, 2, 10],\n",
    "                           [4, 10, 2], [10, 2, 4], [10, 4, 2]])\n",
    "    elif num_layers == 4:\n",
    "        poolsize = trial.suggest_categorical(\n",
    "            'poolsize_4', [[2, 2, 2, 10], [2, 2, 10, 2], [2, 10, 2, 2], [10, 2, 2, 2],\n",
    "                           [2, 2, 4, 5], [2, 2, 5, 4], [2, 4, 2, 5],\n",
    "                           [2, 4, 5, 2], [2, 5, 2, 4], [2, 5, 4, 2],\n",
    "                           [4, 2, 2, 5], [4, 2, 5, 2], [4, 5, 2, 2],\n",
    "                           [5, 2, 2, 4], [5, 2, 4, 2], [5, 4, 2, 2]])\n",
    "    elif num_layers == 5:\n",
    "        poolsize = trial.suggest_categorical(\n",
    "            'poolsize_5', [[2, 2, 2, 2, 5], [2, 2, 2, 5, 2], [2, 2, 5, 2, 2],\n",
    "                           [2, 5, 2, 2, 2], [5, 2, 2, 2, 2]])\n",
    "\n",
    "    channels = [input_dim,]\n",
    "    for i in range(num_layers - 1):\n",
    "        channels.append(trial.suggest_int(f'channels_{i}', 1, 12))\n",
    "    channels.append(output_dim)\n",
    "\n",
    "    kernel_sizes = [trial.suggest_int(\n",
    "        f'kernel_size_{i}', 2, 24) for i in range(num_layers)]\n",
    "\n",
    "    dilations = [trial.suggest_categorical(\n",
    "        f'dilation_{i}', [1, 2, 4]) for i in range(num_layers)]\n",
    "\n",
    "    paddings = [x * (y - 1)//2 for x, y in zip(dilations, kernel_sizes)]\n",
    "\n",
    "    activations = [trial.suggest_categorical(\n",
    "        f'activation_{i}', ['nn.Softplus',\n",
    "                            'nn.SELU',\n",
    "                            'nn.SiLU',\n",
    "                            'nn.Tanh']) for i in range(num_layers)]\n",
    "\n",
    "    return [num_layers, poolsize, channels, kernel_sizes, dilations, paddings, activations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-02 02:14:41,764] A new study created in memory with name: no-name-70850752-0b40-461e-bfcf-ba49f93b2701\n",
      "[I 2024-05-02 02:14:41,769] Trial 0 finished with value: 0.0 and parameters: {'num_layers': 5, 'poolsize_5': [2, 2, 2, 5, 2], 'channels_0': 9, 'channels_1': 9, 'channels_2': 4, 'channels_3': 10, 'kernel_size_0': 17, 'kernel_size_1': 7, 'kernel_size_2': 16, 'kernel_size_3': 20, 'kernel_size_4': 2, 'dilation_0': 1, 'dilation_1': 1, 'dilation_2': 1, 'dilation_3': 4, 'dilation_4': 4, 'activation_0': 'nn.Softplus', 'activation_1': 'nn.Softplus', 'activation_2': 'nn.Softplus', 'activation_3': 'nn.SiLU', 'activation_4': 'nn.SELU'}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-05-02 02:14:41,772] Trial 1 finished with value: 0.0 and parameters: {'num_layers': 3, 'poolsize_3': [2, 10, 4], 'channels_0': 5, 'channels_1': 2, 'kernel_size_0': 20, 'kernel_size_1': 16, 'kernel_size_2': 19, 'dilation_0': 4, 'dilation_1': 1, 'dilation_2': 1, 'activation_0': 'nn.Softplus', 'activation_1': 'nn.SiLU', 'activation_2': 'nn.Tanh'}. Best is trial 0 with value: 0.0.\n",
      "[W 2024-05-02 02:14:41,773] Trial 2 failed with parameters: {'num_layers': 3} because of the following error: ValueError('CategoricalDistribution does not support dynamic value space.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tux/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_170254/621735794.py\", line 6, in objective\n",
      "    num_layers, poolsize, channels, kernel_sizes, dilations, paddings, activations = search_space(\n",
      "                                                                                     ^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_170254/2302214443.py\", line 10, in search_space\n",
      "    poolsize = trial.suggest_categorical(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tux/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 404, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tux/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py\", line 635, in _suggest\n",
      "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
      "  File \"/home/tux/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/storages/_in_memory.py\", line 187, in set_trial_param\n",
      "    distributions.check_distribution_compatibility(\n",
      "  File \"/home/tux/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/distributions.py\", line 670, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n",
      "[W 2024-05-02 02:14:41,774] Trial 2 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CategoricalDistribution does not support dynamic value space.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[101], line 6\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Define the hyperparameter search space\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     num_layers, poolsize, channels, kernel_sizes, dilations, paddings, activations \u001b[38;5;241m=\u001b[39m \u001b[43msearch_space\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     activations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28meval\u001b[39m(activation) \u001b[38;5;28;01mfor\u001b[39;00m activation \u001b[38;5;129;01min\u001b[39;00m activations]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Define the autoencoder\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 10\u001b[0m, in \u001b[0;36msearch_space\u001b[0;34m(trial, input_dim, output_dim)\u001b[0m\n\u001b[1;32m      5\u001b[0m     poolsize \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoolsize_2\u001b[39m\u001b[38;5;124m'\u001b[39m, [[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m16\u001b[39m], [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m      6\u001b[0m                                                         [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m], [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m      7\u001b[0m                                                         [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m20\u001b[39m], [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m      8\u001b[0m                                                         [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m40\u001b[39m], [\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_layers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     poolsize \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoolsize_3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_layers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     18\u001b[0m     poolsize \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoolsize_4\u001b[39m\u001b[38;5;124m'\u001b[39m, [[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     20\u001b[0m                        [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m     21\u001b[0m                        [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     22\u001b[0m                        [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     23\u001b[0m                        [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]])\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:404\u001b[0m, in \u001b[0;36mTrial.suggest_categorical\u001b[0;34m(self, name, choices)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Suggest a value for the categorical parameter.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03mThe value is sampled from ``choices``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    :ref:`configurations` tutorial describes more details and flexible usages.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# There is no need to call self._check_distribution because\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# CategoricalDistribution does not support dynamic value space.\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategoricalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/trial/_trial.py:635\u001b[0m, in \u001b[0;36mTrial._suggest\u001b[0;34m(self, name, distribution)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# `param_value` is validated here (invalid value like `np.nan` raises ValueError).\u001b[39;00m\n\u001b[1;32m    634\u001b[0m param_value_in_internal_repr \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mto_internal_repr(param_value)\n\u001b[0;32m--> 635\u001b[0m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_in_internal_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mdistributions[name] \u001b[38;5;241m=\u001b[39m distribution\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mparams[name] \u001b[38;5;241m=\u001b[39m param_value\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/storages/_in_memory.py:187\u001b[0m, in \u001b[0;36mInMemoryStorage.set_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Check param distribution compatibility with previous trial(s).\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mparam_distribution:\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_distribution_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_studies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstudy_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distribution\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Set param distribution.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mparam_distribution[param_name] \u001b[38;5;241m=\u001b[39m distribution\n",
      "File \u001b[0;32m~/hyper-param-optim-4-vector-field-clustering/.venv/lib/python3.12/site-packages/optuna/distributions.py:670\u001b[0m, in \u001b[0;36mcheck_distribution_compatibility\u001b[0;34m(dist_old, dist_new)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_old \u001b[38;5;241m!=\u001b[39m dist_new:\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    671\u001b[0m         CategoricalDistribution\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not support dynamic value space.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    672\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: CategoricalDistribution does not support dynamic value space."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    num_layers, poolsize, channels, kernel_sizes, dilations, paddings, activations = search_space(\n",
    "        trial, input_dim=3, output_dim=3)\n",
    "    activations = [eval(activation) for activation in activations]\n",
    "    \n",
    "    # Define the autoencoder\n",
    "    autoencoder = Autoencoder(num_layers=num_layers,\n",
    "                              poolsize=poolsize,\n",
    "                              channels=channels,\n",
    "                              kernel_sizes=kernel_sizes,\n",
    "                              dilations=dilations,\n",
    "                              paddings=paddings,\n",
    "                              activations=activations,\n",
    "                              epochs=10,\n",
    "                              batch_size=32,\n",
    "                              learning_rate=1e-3,\n",
    "                              data=train_set)\n",
    "    \n",
    "    print('fin')\n",
    "\n",
    "    return 0\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
